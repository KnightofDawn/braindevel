{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance on the Kaggle Grasp Lift set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short:\n",
    "We are at 0.97 AUC , best reported single model of the competition was at 0.976 AUC. \n",
    "\n",
    "Our best model uses (leaky) relu, max pooling, fairly long filters (30) and Network in Network-like layers (with identity activation, so it is not really motivatable in same way as network in network). \n",
    "\n",
    "I will show performances of different models and parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-Layer Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic architecture\n",
    "\n",
    "Convolutions with time length 30, followed by poolings. 5 Layers refers to number of conv+pool parts.\n",
    "\n",
    "Everything in batch x channel x time x 1 (But note: Initially 32 input sensors/channels are in dimension **1**, not in the channel dimension!)\n",
    "\n",
    "|#Layer|Layer|Filter Size| Filter Stride|Nonlinearity/PoolingMode|\n",
    "|-|\n",
    "|1|Conv|30x1|1x1|Identity|\n",
    "|2|Conv|1x32|1x1|Leaky Relu|\n",
    "|3|Pool|3x1|3x1|Max|\n",
    "|4|Conv|30x1|1x1|Leaky Relu|\n",
    "|5|Pool|3x1|3x1|Max|\n",
    "|6|Conv|30x1|1x1|Leaky Relu|\n",
    "|7|Pool|3x1|3x1|Max|\n",
    "|8|Conv|30x1|1x1|Leaky Relu|\n",
    "|9|Pool|3x1|3x1|Max|\n",
    "|10|Conv|30x1|1x1|Leaky Relu|\n",
    "|11|Pool|3x1|3x1|Max|\n",
    "|12|Conv|1x1|1x1|Softmax|\n",
    "\n",
    "(all layers have 40 filters)\n",
    "\n",
    "The last layer can also be seen as a fully connected layer and the 1x1 filter size determined by the input length (the input of 3752 samples is already reduced to only length 1 in the time dimension at that layer). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different activations and poolings\n",
    "\n",
    "Tests still done before substantial improvements by training on all subjects:\n",
    "\n",
    "Leaky Relu and max pooling was always better than squaring + log + mean pooling.\n",
    "\n",
    "|Activation + Pooling| Test AUC|\n",
    "|-|-|\n",
    "|Leaky Relu + Max|92.7|\n",
    "|Square + Mean + Log first layer|89.7|\n",
    "|Square + Mean + Log later layers|79.1|\n",
    "\n",
    "Square + Max Pooling not tested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization, filter lengths, more filters/layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training on all subjects, we get better results. Here an overview:\n",
    "\n",
    "Base version is a five layer net with our (#time_length x 1) + (1 x #channels) two-layers \"separated\" convolution in the first layer. Later layers normal convolutions. Filter lengths 30(!) in all layers. Max Pooling between all layers, with length=stride=3 (for 6 layers and 7 layers, length=stride=2).\n",
    "\n",
    "|Comment| Test AUC| Kaggle AUC|\n",
    "|-|\n",
    "|Base version|95.23|96.1|\n",
    "|Batch normalization|96.22|96.35|\n",
    "|8,12,20 filter time lengths|<95.5||\n",
    "|80 filters | 96.34|96.7|\n",
    "|6 Layer (40 filters)|96.36|-|\n",
    "|7 Layer (40 filters)| 96.02|96.15|\n",
    "|NiN-like (80 filters)|96.45|96.6|\n",
    "\n",
    "Everything below batch normalization was with batch normalization. My last test was a different preprocessing: only remove a baseline mean (before the time window) of every channel (before I also tried to standardize the data to unit variance). This is quite similar to what the second place did. With NiN-like layers and 80 filters this led to\n",
    "\n",
    "|Comment| Test AUC| Kaggle AUC|\n",
    "|-|\n",
    "|Remove baseline mean|96.53|97.0|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Fairly standard architectures can lead to quite good performance, batch normalization seems to help. \n",
    "\n",
    "It seems quite probable that NiN-like etc. is not necessary. For example, 6 Layers with 80 filters and remove baseline mean might also lead to ~97 Kaggle performance. \n",
    "There are some quite obvious things to test which I am not so interested in since I want to focus on visualizations:\n",
    "\n",
    "* higher number of filters in earlier layers, smaller in later layers\n",
    "* longer filters in earlier layers, shorter in later layers\n",
    "* try smaller nets again with the improvements (training all subjects, batch normalization, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
