{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance on the Kaggle Grasp Lift set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short:\n",
    "We are at 0.97 AUC , best reported single model of the competition was at 0.976 AUC. \n",
    "\n",
    "Our best model uses (leaky) relu, max pooling, fairly long filters (30) and 1x1-convs before normal convs in later  layers (with identity activation inbetween). \n",
    "\n",
    "I will show performances of different models and parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-Layer Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic architecture\n",
    "\n",
    "Convolutions with time length 30, followed by poolings. 5 Layers refers to number of conv+pool parts.\n",
    "\n",
    "Everything in batch x channel x time x 1 (But note: Initially 32 input sensors/channels are in dimension **1**, not in the channel dimension!)\n",
    "\n",
    "|#Layer|Layer|Filter Size| Filter Stride|Nonlinearity/PoolingMode|\n",
    "|-|\n",
    "|1|Conv|30x1|1x1|Identity|\n",
    "|2|Conv|1x32|1x1|Leaky Relu|\n",
    "|3|Pool|3x1|3x1|Max|\n",
    "|4|Conv|30x1|1x1|Leaky Relu|\n",
    "|5|Pool|3x1|3x1|Max|\n",
    "|6|Conv|30x1|1x1|Leaky Relu|\n",
    "|7|Pool|3x1|3x1|Max|\n",
    "|8|Conv|30x1|1x1|Leaky Relu|\n",
    "|9|Pool|3x1|3x1|Max|\n",
    "|10|Conv|30x1|1x1|Leaky Relu|\n",
    "|11|Pool|3x1|3x1|Max|\n",
    "|12|Conv|1x1|1x1|Softmax|\n",
    "\n",
    "(all layers have 40 filters)\n",
    "\n",
    "The last layer can also be seen as a fully connected layer and the 1x1 filter size determined by the input length (the input of 3752 samples is already reduced to only length 1 in the time dimension at that layer). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different activations and poolings\n",
    "\n",
    "Tests still done before substantial improvements by training on all subjects:\n",
    "\n",
    "Leaky Relu and max pooling was always better than squaring + log + mean pooling.\n",
    "\n",
    "|Activation + Pooling| Test AUC|\n",
    "|-|-|\n",
    "|Leaky Relu + Max|92.7|\n",
    "|Square + Mean + Log first layer|89.7|\n",
    "|Square + Mean + Log later layers|79.1|\n",
    "\n",
    "Square + Max Pooling not tested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization, filter lengths, more filters/layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training on all subjects, we get better results. Here an overview:\n",
    "\n",
    "Base version is a five layer net with our (#time_length x 1) + (1 x #channels) two-layers \"separated\" convolution in the first layer. Later layers normal convolutions. Filter lengths 30(!) in all layers. Max Pooling between all layers, with length=stride=3 (for 6 layers and 7 layers, length=stride=2).\n",
    "\n",
    "|Comment| Test AUC| Kaggle AUC|\n",
    "|-|\n",
    "|Base version|95.23|96.1|\n",
    "|Batch normalization|96.22|96.35|\n",
    "|8,12,20 filter time lengths|<95.5||\n",
    "|80 filters | 96.34|96.7|\n",
    "|6 Layer (40 filters)|96.36|-|\n",
    "|7 Layer (40 filters)| 96.02|96.15|\n",
    "|pre-1x1-conv (80 filters)|96.45|96.6|\n",
    "\n",
    "Everything below batch normalization was with batch normalization. My last test was a different preprocessing: only remove a baseline mean (before the time window) of every channel (before I also tried to standardize the data to unit variance). This is quite similar to what the second place did. With 1x1 conv before each conv and 80 filters this led to\n",
    "\n",
    "|Comment| Test AUC| Kaggle AUC|\n",
    "|-|\n",
    "|Remove baseline mean|96.53|97.0|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Fairly standard architectures can lead to quite good performance, batch normalization seems to help. \n",
    "\n",
    "It seems quite probable that pre-1x1-conv is not necessary. For example, 6 Layers with 80 filters and remove baseline mean might also lead to ~97 Kaggle performance. \n",
    "There are some quite obvious things to test which I am not so interested in since I want to focus on visualizations:\n",
    "\n",
    "* higher number of filters in earlier layers, smaller in later layers\n",
    "* longer filters in earlier layers, shorter in later layers\n",
    "* try smaller nets again with the improvements (training all subjects, batch normalization, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Conclusion (more filters, truly separable conv) :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* more filters than 80 decrease accuracy ~0.5%\n",
    "* non-separable conv in first layer decreases accuracy ~1.5%\n",
    "* Increase of batch size from 42 to 45 increases accuracy ~0.3%\n",
    "\n",
    "All below the first are with batch normalization, removing baseline, etc\n",
    "(first result is copied from above for comparison)\n",
    "\n",
    "|Comment| Test AUC| Kaggle AUC|\n",
    "|-|\n",
    "|80 filters, first time, then spatial, rest normal|96.3|96.7|\n",
    "|80 Filters pre-1x1-conv, batchsize 45|96.8||\n",
    "|120 Filters pre-1x1-conv, batchsize 45|96.3||\n",
    "|140 Filters pre-1x1-conv, batchsize 45|96.2||\n",
    "|Normal conv all layers (including first), batchsize 45|94.8||\n",
    "|first real separable conv, then normal convs, batchsize 45|95.4||\n",
    "|first normal conv, then pre-1x1-conv, batchsize 45|96.3||"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
